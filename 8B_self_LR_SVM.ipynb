{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print(statement, arguments, do_print = True):\n",
    "    \n",
    "    if do_print:\n",
    "        print(statement.format(*arguments))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lb(character, num = 60):\n",
    "\n",
    "    print(character*num)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 rows of DataFrame : \n",
      "\n",
      "   index           f1            f2        f3    y\n",
      "0      0  -195.871045 -14843.084171  5.532140  1.0\n",
      "1      1 -1217.183964  -4068.124621  4.416082  1.0\n",
      "2      2     9.138451   4413.412028  0.425317  0.0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('task_b.csv')\n",
    "_print(\"Top {} rows of DataFrame : \\n\\n{}\", [3, data.head(n = 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          index        f1        f2        f3         y\n",
      "index  1.000000  0.178685  0.149585 -0.017404 -0.014203\n",
      "f1     0.178685  1.000000  0.065468  0.123589  0.067172\n",
      "f2     0.149585  0.065468  1.000000 -0.055561 -0.017944\n",
      "f3    -0.017404  0.123589 -0.055561  1.000000  0.839060\n",
      "y     -0.014203  0.067172 -0.017944  0.839060  1.000000\n"
     ]
    }
   ],
   "source": [
    "data_corr = data.corr()\n",
    "print(data_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index       57.879185\n",
      "f1         488.195035\n",
      "f2       10403.417325\n",
      "f3           2.926662\n",
      "y            0.501255\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data_std = data.std()\n",
    "print(data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if our features are with different variance \n",
    "\n",
    "<pre>\n",
    "* <b>As part of this task you will observe how linear models work in case of data having feautres with different variance</b>\n",
    "* <b>from the output of the above cells you can observe that var(F2)>>var(F1)>>Var(F3)</b>\n",
    "\n",
    "> <b>Task1</b>:\n",
    "    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' and check the feature importance\n",
    "    2. Apply SVM(SGDClassifier with hinge) on 'data' and check the feature importance\n",
    "\n",
    "> <b>Task2</b>:\n",
    "    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' after standardization \n",
    "       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n",
    "    2. Apply SVM(SGDClassifier with hinge) on 'data' after standardization \n",
    "       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.1] Logistic regression(SGDClassifier with logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lr_clsfr: Logistic Rgression classifier\n",
    "\"\"\"\n",
    "lr_clsfr = linear_model.SGDClassifier(loss='log', eta0 = 0.0001, alpha = 0.0001, penalty = 'l2', random_state = 15, verbose = 2, n_jobs = -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Data Type of d_train: <class 'numpy.ndarray'>\n",
      "2. Shape of d_train: (200, 3)\n",
      "\n",
      "\n",
      "1. Data Type of y: <class 'numpy.ndarray'>\n",
      "2. Shape of y: \n"
     ]
    }
   ],
   "source": [
    "d_train = data[['f1', 'f2', 'f3']].values\n",
    "_print(\"1. Data Type of d_train: {}\", [type(d_train)])\n",
    "_print(\"2. Shape of d_train: {}\\n\\n\", [d_train.shape])\n",
    "\n",
    "y = data[['y']].values\n",
    "_print(\"1. Data Type of y: {}\", [type(y)])\n",
    "_print(\"2. Shape of y: \", [y.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's reshape y dimension from 2-D array to vector\n",
    "\"\"\"\n",
    "y = y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 89870.22, NNZs: 3, Bias: -155.051327, T: 200, Avg. loss: 227510618.879137\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 43791.04, NNZs: 3, Bias: -144.898405, T: 400, Avg. loss: 200670830.244106\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 21826.51, NNZs: 3, Bias: -167.580253, T: 600, Avg. loss: 220221238.438343\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 44052.83, NNZs: 3, Bias: -220.577819, T: 800, Avg. loss: 179903697.744537\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 43829.42, NNZs: 3, Bias: -246.905532, T: 1000, Avg. loss: 159178154.202827\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 45138.87, NNZs: 3, Bias: -208.546352, T: 1200, Avg. loss: 142129650.458868\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 27699.02, NNZs: 3, Bias: -168.679560, T: 1400, Avg. loss: 134776289.480256\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 30003.76, NNZs: 3, Bias: -147.095138, T: 1600, Avg. loss: 122695625.336222\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 40159.27, NNZs: 3, Bias: -173.114320, T: 1800, Avg. loss: 105392821.686273\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 18380.20, NNZs: 3, Bias: -173.985270, T: 2000, Avg. loss: 109406423.687877\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 33614.14, NNZs: 3, Bias: -151.697668, T: 2200, Avg. loss: 104611163.295840\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 28081.35, NNZs: 3, Bias: -167.072934, T: 2400, Avg. loss: 84531652.656471\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 49666.61, NNZs: 3, Bias: -189.824733, T: 2600, Avg. loss: 90663895.072324\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 57159.43, NNZs: 3, Bias: -155.164527, T: 2800, Avg. loss: 73300728.123771\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 24516.31, NNZs: 3, Bias: -142.245980, T: 3000, Avg. loss: 61143613.980263\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 15015.86, NNZs: 3, Bias: -159.140627, T: 3200, Avg. loss: 71025642.232696\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 20462.44, NNZs: 3, Bias: -168.751459, T: 3400, Avg. loss: 62715215.539027\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 28659.44, NNZs: 3, Bias: -171.142036, T: 3600, Avg. loss: 59523726.187933\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 26592.00, NNZs: 3, Bias: -175.112527, T: 3800, Avg. loss: 60215259.923806\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 15589.86, NNZs: 3, Bias: -193.384867, T: 4000, Avg. loss: 56474001.620423\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 16065.45, NNZs: 3, Bias: -213.149785, T: 4200, Avg. loss: 48442814.077230\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 14069.21, NNZs: 3, Bias: -190.144797, T: 4400, Avg. loss: 50351240.172419\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 9920.91, NNZs: 3, Bias: -188.148064, T: 4600, Avg. loss: 43039365.439501\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 12013.89, NNZs: 3, Bias: -195.017494, T: 4800, Avg. loss: 54210598.721807\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 30388.34, NNZs: 3, Bias: -191.521585, T: 5000, Avg. loss: 41931376.234027\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 18244.54, NNZs: 3, Bias: -199.584009, T: 5200, Avg. loss: 42741437.587700\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 18262.56, NNZs: 3, Bias: -193.113214, T: 5400, Avg. loss: 47736667.233234\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 23136.16, NNZs: 3, Bias: -193.039368, T: 5600, Avg. loss: 52024855.120507\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 30608.20, NNZs: 3, Bias: -199.023428, T: 5800, Avg. loss: 36416617.232855\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 15322.15, NNZs: 3, Bias: -208.758108, T: 6000, Avg. loss: 37024768.659542\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 17545.75, NNZs: 3, Bias: -194.639380, T: 6200, Avg. loss: 46403405.008290\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 11577.57, NNZs: 3, Bias: -207.079475, T: 6400, Avg. loss: 37144450.506530\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 12692.90, NNZs: 3, Bias: -212.405176, T: 6600, Avg. loss: 29195403.408555\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 23239.73, NNZs: 3, Bias: -221.505585, T: 6800, Avg. loss: 30650521.658633\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 11221.75, NNZs: 3, Bias: -219.064031, T: 7000, Avg. loss: 33110713.361357\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 11562.46, NNZs: 3, Bias: -219.026462, T: 7200, Avg. loss: 30476915.977737\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 12162.85, NNZs: 3, Bias: -226.269056, T: 7400, Avg. loss: 35771824.479969\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 19564.70, NNZs: 3, Bias: -239.124737, T: 7600, Avg. loss: 36370991.813773\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 38 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
       "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=-1,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clsfr.fit(d_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3925.14601273, -16033.05764291,  10502.94022174]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clsfr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-239.12473731])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clsfr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.2] SVM(SGDClassifier with hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "svm_clsfr: Support Vector Machine classifier\n",
    "\"\"\"\n",
    "svm_clsfr = linear_model.SGDClassifier(loss='hinge', eta0 = 0.0001, alpha = 0.0001, penalty = 'l2', random_state = 15, verbose = 2, n_jobs = -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 50970.78, NNZs: 3, Bias: -139.425296, T: 200, Avg. loss: 237646740.055412\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 48251.18, NNZs: 3, Bias: -107.540533, T: 400, Avg. loss: 198997629.606033\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 47283.82, NNZs: 3, Bias: -98.147510, T: 600, Avg. loss: 221678301.804550\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 42763.27, NNZs: 3, Bias: -203.499872, T: 800, Avg. loss: 184224165.832094\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 46523.86, NNZs: 3, Bias: -208.473184, T: 1000, Avg. loss: 163252412.932515\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 19449.28, NNZs: 3, Bias: -203.212119, T: 1200, Avg. loss: 144245644.752882\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 28910.00, NNZs: 3, Bias: -180.696759, T: 1400, Avg. loss: 130092109.903065\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 34971.40, NNZs: 3, Bias: -163.150693, T: 1600, Avg. loss: 121277810.479388\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 33276.26, NNZs: 3, Bias: -178.221644, T: 1800, Avg. loss: 105218983.318030\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 18314.88, NNZs: 3, Bias: -168.748680, T: 2000, Avg. loss: 105751075.568105\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 34026.26, NNZs: 3, Bias: -137.000345, T: 2200, Avg. loss: 104100927.952489\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 32060.64, NNZs: 3, Bias: -164.390101, T: 2400, Avg. loss: 85184284.955179\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 33408.49, NNZs: 3, Bias: -161.254990, T: 2600, Avg. loss: 94393954.300818\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 57260.17, NNZs: 3, Bias: -140.340518, T: 2800, Avg. loss: 69341260.386531\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 25327.28, NNZs: 3, Bias: -127.421971, T: 3000, Avg. loss: 61111146.019126\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 29918.51, NNZs: 3, Bias: -151.478787, T: 3200, Avg. loss: 72048575.772406\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 25927.67, NNZs: 3, Bias: -163.617817, T: 3400, Avg. loss: 56499951.273764\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 26840.99, NNZs: 3, Bias: -159.252208, T: 3600, Avg. loss: 61377838.704854\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 16611.54, NNZs: 3, Bias: -165.343898, T: 3800, Avg. loss: 63182350.326400\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 17932.89, NNZs: 3, Bias: -181.588087, T: 4000, Avg. loss: 54754912.040320\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 20618.77, NNZs: 3, Bias: -183.713029, T: 4200, Avg. loss: 48531162.067885\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 17582.59, NNZs: 3, Bias: -160.795773, T: 4400, Avg. loss: 49205687.377794\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 12238.11, NNZs: 3, Bias: -151.631232, T: 4600, Avg. loss: 45051759.128370\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 12788.80, NNZs: 3, Bias: -163.778006, T: 4800, Avg. loss: 54258128.144688\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 29238.53, NNZs: 3, Bias: -161.900288, T: 5000, Avg. loss: 47795204.045875\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 17511.09, NNZs: 3, Bias: -168.326384, T: 5200, Avg. loss: 44003343.599421\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 18947.25, NNZs: 3, Bias: -174.559104, T: 5400, Avg. loss: 45122963.443264\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 25130.28, NNZs: 3, Bias: -179.174563, T: 5600, Avg. loss: 51090318.362048\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 30142.91, NNZs: 3, Bias: -186.608547, T: 5800, Avg. loss: 36989720.994604\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 15148.98, NNZs: 3, Bias: -196.343227, T: 6000, Avg. loss: 36986359.783150\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 15145.60, NNZs: 3, Bias: -189.287735, T: 6200, Avg. loss: 39786508.825210\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 11102.35, NNZs: 3, Bias: -189.476491, T: 6400, Avg. loss: 39212165.268714\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 13179.57, NNZs: 3, Bias: -192.017279, T: 6600, Avg. loss: 31809325.675832\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 22709.10, NNZs: 3, Bias: -198.477941, T: 6800, Avg. loss: 29997166.152475\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 11986.41, NNZs: 3, Bias: -203.648269, T: 7000, Avg. loss: 30788036.546377\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 11988.21, NNZs: 3, Bias: -204.837743, T: 7200, Avg. loss: 23108194.706015\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 12579.76, NNZs: 3, Bias: -214.481769, T: 7400, Avg. loss: 34582923.132730\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 24206.08, NNZs: 3, Bias: -222.667517, T: 7600, Avg. loss: 33918946.617544\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 12470.53, NNZs: 3, Bias: -218.000666, T: 7800, Avg. loss: 34052236.991689\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 11512.54, NNZs: 3, Bias: -223.624922, T: 8000, Avg. loss: 35026025.189644\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 11169.92, NNZs: 3, Bias: -213.747039, T: 8200, Avg. loss: 28684903.853204\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 41 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
       "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=-1,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clsfr.fit(d_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1441.65036452, -3083.88512888, 10638.5348658 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clsfr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-213.74703893])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clsfr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2.1] Logistic regression(SGDClassifier with logloss) after standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 rows of DataFrame : \n",
      "\n",
      "   index           f1            f2        f3    y\n",
      "0      0  -195.871045 -14843.084171  5.532140  1.0\n",
      "1      1 -1217.183964  -4068.124621  4.416082  1.0\n",
      "2      2     9.138451   4413.412028  0.425317  0.0\n"
     ]
    }
   ],
   "source": [
    "_print(\"Top {} rows of DataFrame : \\n\\n{}\", [3, data.head(n = 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [2.1.1.1] Standariztion of feature `f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_feature_nda = data['f1'].values.reshape(-1, 1)\n",
    "\n",
    "scaler.fit(f1_feature_nda)\n",
    "data['f1_transform'] = scaler.transform(f1_feature_nda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_feature_nda = data['f2'].values.reshape(-1, 1)\n",
    "\n",
    "scaler.fit(f2_feature_nda)\n",
    "data['f2_transform'] = scaler.transform(f2_feature_nda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3_feature_nda = data['f3'].values.reshape(-1, 1)\n",
    "\n",
    "scaler.fit(f3_feature_nda)\n",
    "data['f3_transform'] = scaler.transform(f3_feature_nda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 rows of DataFrame : \n",
      "\n",
      "   index           f1            f2        f3    y  f1_transform  \\\n",
      "0      0  -195.871045 -14843.084171  5.532140  1.0     -0.423126   \n",
      "1      1 -1217.183964  -4068.124621  4.416082  1.0     -2.520394   \n",
      "2      2     9.138451   4413.412028  0.425317  0.0     -0.002139   \n",
      "\n",
      "   f2_transform  f3_transform  \n",
      "0     -1.555602      0.181651  \n",
      "1     -0.517290     -0.200648  \n",
      "2      0.300020     -1.567659  \n"
     ]
    }
   ],
   "source": [
    "_print(\"Top {} rows of DataFrame : \\n\\n{}\", [3, data.head(n = 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Data Type of d_train_std: <class 'numpy.ndarray'>\n",
      "2. Shape of d_train_std: (200, 3)\n",
      "\n",
      "\n",
      "1. Data Type of y: <class 'numpy.ndarray'>\n",
      "2. Shape of y: \n"
     ]
    }
   ],
   "source": [
    "d_train_std = data[['f1_transform', 'f2_transform', 'f3_transform']].values\n",
    "_print(\"1. Data Type of d_train_std: {}\", [type(d_train_std)])\n",
    "_print(\"2. Shape of d_train_std: {}\\n\\n\", [d_train_std.shape])\n",
    "\n",
    "\n",
    "_print(\"1. Data Type of y: {}\", [type(y)])\n",
    "_print(\"2. Shape of y: \", [y.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 53.31, NNZs: 3, Bias: -5.276812, T: 200, Avg. loss: 1.866743\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 47.34, NNZs: 3, Bias: 3.430650, T: 400, Avg. loss: 1.352641\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 43.99, NNZs: 3, Bias: 2.140512, T: 600, Avg. loss: 1.213044\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 33.54, NNZs: 3, Bias: -5.951509, T: 800, Avg. loss: 1.279978\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 32.19, NNZs: 3, Bias: 1.875443, T: 1000, Avg. loss: 0.999923\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 31.45, NNZs: 3, Bias: -0.998614, T: 1200, Avg. loss: 0.563070\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 32.57, NNZs: 3, Bias: 2.720694, T: 1400, Avg. loss: 0.687991\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 30.51, NNZs: 3, Bias: -2.435934, T: 1600, Avg. loss: 0.722018\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 28.04, NNZs: 3, Bias: -3.174216, T: 1800, Avg. loss: 0.807365\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 23.99, NNZs: 3, Bias: -2.572984, T: 2000, Avg. loss: 0.553149\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 22.28, NNZs: 3, Bias: -2.147460, T: 2200, Avg. loss: 0.411893\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 20.19, NNZs: 3, Bias: 0.627308, T: 2400, Avg. loss: 0.529198\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 21.56, NNZs: 3, Bias: -2.041266, T: 2600, Avg. loss: 0.605205\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 19.78, NNZs: 3, Bias: -1.691095, T: 2800, Avg. loss: 0.510316\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 18.38, NNZs: 3, Bias: 1.544255, T: 3000, Avg. loss: 0.352165\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 19.07, NNZs: 3, Bias: 2.411612, T: 3200, Avg. loss: 0.394782\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 18.00, NNZs: 3, Bias: -2.138986, T: 3400, Avg. loss: 0.306954\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 16.71, NNZs: 3, Bias: -5.620388, T: 3600, Avg. loss: 0.323921\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 17.02, NNZs: 3, Bias: 0.057260, T: 3800, Avg. loss: 0.372382\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 14.89, NNZs: 3, Bias: -0.230625, T: 4000, Avg. loss: 0.374575\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 13.71, NNZs: 3, Bias: -2.768940, T: 4200, Avg. loss: 0.462690\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 13.42, NNZs: 3, Bias: 0.939680, T: 4400, Avg. loss: 0.275341\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 12.14, NNZs: 3, Bias: -2.044632, T: 4600, Avg. loss: 0.352880\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 12.77, NNZs: 3, Bias: -2.295930, T: 4800, Avg. loss: 0.378551\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 13.33, NNZs: 3, Bias: -1.451579, T: 5000, Avg. loss: 0.187752\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 12.28, NNZs: 3, Bias: -0.251063, T: 5200, Avg. loss: 0.334557\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 12.67, NNZs: 3, Bias: 0.332995, T: 5400, Avg. loss: 0.328642\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 11.20, NNZs: 3, Bias: -0.431264, T: 5600, Avg. loss: 0.321415\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 11.61, NNZs: 3, Bias: -1.026621, T: 5800, Avg. loss: 0.273815\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 10.38, NNZs: 3, Bias: -0.726479, T: 6000, Avg. loss: 0.353289\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 30 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
       "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=-1,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clsfr.fit(d_train_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.29741788, -0.66973479, 10.35436789]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clsfr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.72647926])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clsfr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2.2] SVM(SGDClassifier with hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 42.77, NNZs: 3, Bias: -0.401235, T: 200, Avg. loss: 2.007617\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 41.12, NNZs: 3, Bias: -0.275640, T: 400, Avg. loss: 1.330494\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 34.84, NNZs: 3, Bias: 0.082357, T: 600, Avg. loss: 1.548790\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 33.91, NNZs: 3, Bias: -5.367733, T: 800, Avg. loss: 1.324727\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 33.04, NNZs: 3, Bias: -0.044521, T: 1000, Avg. loss: 0.970131\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 33.35, NNZs: 3, Bias: -4.658497, T: 1200, Avg. loss: 0.786032\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 33.34, NNZs: 3, Bias: 4.123484, T: 1400, Avg. loss: 1.002599\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 35.12, NNZs: 3, Bias: -0.193457, T: 1600, Avg. loss: 0.904594\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 32.42, NNZs: 3, Bias: -3.925287, T: 1800, Avg. loss: 1.007940\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 30.21, NNZs: 3, Bias: -0.390801, T: 2000, Avg. loss: 0.937491\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 27.78, NNZs: 3, Bias: -0.439292, T: 2200, Avg. loss: 0.573288\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 27.34, NNZs: 3, Bias: -0.483781, T: 2400, Avg. loss: 0.769558\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 25.69, NNZs: 3, Bias: -3.347256, T: 2600, Avg. loss: 0.878631\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 25.03, NNZs: 3, Bias: -0.697268, T: 2800, Avg. loss: 0.586768\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 22.47, NNZs: 3, Bias: -0.729693, T: 3000, Avg. loss: 0.657703\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 22.51, NNZs: 3, Bias: -0.774151, T: 3200, Avg. loss: 0.580925\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 16 epochs took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
       "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=-1,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clsfr.fit(d_train_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1441.65036452, -3083.88512888, 10638.5348658 ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clsfr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-213.74703893])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clsfr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
